{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caaa79a3",
   "metadata": {},
   "source": [
    "Here is an example on how to use PySpark to do a few simple data manipulations and create a csv as output. This notebook gives the same output as the Pandas and PETL examples.\n",
    "\n",
    "I used the pyspark-notebook docker image provided by Jupyter to run this notebook:https://hub.docker.com/r/jupyter/pyspark-notebook. \n",
    "\n",
    "You can run the docker environment as follows:\n",
    "docker run -v \\<local path to this notebook\\>:/home/jovyan/work -p 8888:8888 jupyter/pyspark-notebook\n",
    "    \n",
    "Author: PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05cab4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import upper, udf\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c33ea0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "   .master(\"local\") \\\n",
    "   .appName(\"Join Example\") \\\n",
    "   .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "   .getOrCreate()\n",
    "   \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29f9278f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- game_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- score: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- loggedin: string (nullable = true)\n",
      "\n",
      "+---+-------+---------+-----+------+----------+\n",
      "| id|game_id|     name|score|amount|  loggedin|\n",
      "+---+-------+---------+-----+------+----------+\n",
      "|  1|     10|Mega Claw|   10|    34|20-01-1980|\n",
      "|  2|     10|   Pewter|   24|   223|23-02-1985|\n",
      "|  3|     10|Sir Tiger|  500|  5632|11-05-1990|\n",
      "|  4|     11|Sir Tiger|   50|    10|01-01-1901|\n",
      "+---+-------+---------+-----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_scores = spark.read.options(header='True', inferSchema='True').csv(\"inputfiles/players.csv\")\n",
    "df_scores.printSchema()\n",
    "df_scores.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99c3adcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- game_id: integer (nullable = true)\n",
      " |-- game_name: string (nullable = true)\n",
      " |-- game_type: string (nullable = true)\n",
      "\n",
      "+-------+----------+---------+\n",
      "|game_id| game_name|game_type|\n",
      "+-------+----------+---------+\n",
      "|     10|Mouse Hunt|        R|\n",
      "|     11|Bird Catch|        A|\n",
      "|     12|Fast Racer|        S|\n",
      "+-------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_games = spark.read.options(header='True', inferSchema='True').csv(\"inputfiles/games.csv\")\n",
    "df_games.printSchema()\n",
    "df_games.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8648177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---------+-----+------+----------+-------+----------+---------+\n",
      "| id|game_id|     name|score|amount|  loggedin|game_id| game_name|game_type|\n",
      "+---+-------+---------+-----+------+----------+-------+----------+---------+\n",
      "|  1|     10|Mega Claw|   10|    34|20-01-1980|     10|Mouse Hunt|        R|\n",
      "|  2|     10|   Pewter|   24|   223|23-02-1985|     10|Mouse Hunt|        R|\n",
      "|  3|     10|Sir Tiger|  500|  5632|11-05-1990|     10|Mouse Hunt|        R|\n",
      "|  4|     11|Sir Tiger|   50|    10|01-01-1901|     11|Bird Catch|        A|\n",
      "+---+-------+---------+-----+------+----------+-------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join = df_scores.join(df_games, df_scores.game_id == df_games.game_id, \"inner\")\n",
    "df_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78bee463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_game_type(game_type):\n",
    "    mapping = {\"A\": \"Action\", \"R\": \"RPG\", \"S\": \"Simulation\"}\n",
    "\n",
    "    return mapping[game_type]\n",
    "\n",
    "udf_conv_game_type = udf(conv_game_type, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12c21009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date(dt):\n",
    "    dt_parts = dt.split('-')\n",
    "    return f\"{dt_parts[2]}{dt_parts[1]}{dt_parts[0]}\"\n",
    "\n",
    "udf_convert_date = udf(convert_date, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0edf6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+--------------+---------+\n",
      "|     name| game_name|total_score|date_logged_in|game_type|\n",
      "+---------+----------+-----------+--------------+---------+\n",
      "|MEGA CLAW|Mouse Hunt|        340|      19800120|      RPG|\n",
      "|   PEWTER|Mouse Hunt|       5352|      19850223|      RPG|\n",
      "|SIR TIGER|Mouse Hunt|    2816000|      19900511|      RPG|\n",
      "|SIR TIGER|Bird Catch|        500|      19010101|   Action|\n",
      "+---------+----------+-----------+--------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final = df_join \\\n",
    "    .withColumn(\"totalscore\", df_join.score * df_join.amount) \\\n",
    "    .withColumn(\"name\", upper(\"name\")) \\\n",
    "    .withColumn(\"game_type\", udf_conv_game_type(\"game_type\")) \\\n",
    "    .withColumn(\"loggedin\", udf_convert_date(\"loggedin\")) \\\n",
    "    .select('name', 'game_name', 'totalscore', 'loggedin', 'game_type') \\\n",
    "    .withColumnRenamed('loggedin', 'date_logged_in') \\\n",
    "    .withColumnRenamed('totalscore', 'total_score')\n",
    "\n",
    "\n",
    "df_final.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca597c7",
   "metadata": {},
   "source": [
    "Save as a csv in folder game_score. By using repartition(1) there is only 1 csv file. This will move all the data to a single machine first so is not suitable for very large datasets.\n",
    "\n",
    "The name of the file is chosen by PySpark (for example: part-00000-cf2aab4b-3b79-4342-849a-bbdf7aff4288-c000.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fda5acc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.repartition(1).write.options(header='True', delimiter='|').format('csv').mode('overwrite').save(\"game_scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4220ee",
   "metadata": {},
   "source": [
    "Alternative: Save as a single CSV by using Pandas (this will move all data to a single machine as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56d9f3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.toPandas().to_csv(\"game_scores.csv\", index=False, header=True, sep='|')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
